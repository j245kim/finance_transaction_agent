{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_item_links(html_path):\n",
    "    # HTML 파일 읽기\n",
    "    with open(html_path, 'r', encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "\n",
    "    # &nbsp; 제거 및 공백 변환\n",
    "    html = re.sub(r'(&nbsp;)+', ' ', html)\n",
    "\n",
    "    # \"ITEM X.\" 패턴 찾기 (예: ITEM 1., ITEM 2., ITEM 5.)\n",
    "    found_items = re.findall(r'item\\s+?\\d\\.', html, flags=re.IGNORECASE)\n",
    "    print(\"추출된 ITEM 목록:\", found_items)\n",
    "\n",
    "    # 특정 인덱스의 ITEM 정보 가져오기\n",
    "    selected_indices = [0, 1, 4, 7]  # 1번째, 2번째, 5번째, 8번째 (0-based index)\n",
    "    selected_items = [found_items[i] for i in selected_indices if i < len(found_items)]\n",
    "    print(\"선택된 ITEM 목록:\", selected_items)\n",
    "\n",
    "    # HTML 파싱\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 선택된 ITEM과 동일한 이름을 가진 <a> 태그 찾기\n",
    "    item_links = []\n",
    "    for item in selected_items:\n",
    "        matching_link = soup.find('a', string=re.compile(re.escape(item), re.IGNORECASE))\n",
    "        if matching_link:\n",
    "            href_value = matching_link.get('href')\n",
    "            if href_value and href_value.startswith('#'):\n",
    "                href_value = href_value[1:]  # '#' 제거\n",
    "            item_links.append(href_value)\n",
    "        else:\n",
    "            item_links.append(None)\n",
    "\n",
    "    # ranges 리스트 구성\n",
    "    ranges = [(item_links[0], item_links[1]), (item_links[2], item_links[3])]\n",
    "    \n",
    "    return soup, ranges\n",
    "\n",
    "def extract_table_from_html(table, table_index):\n",
    "    \"\"\" HTML 테이블을 판다스 DataFrame으로 변환하고 Markdown 형식으로 반환 \"\"\"\n",
    "    try:\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        print(df)\n",
    "        return f\"### {table_index}\\n\" + df.to_markdown(index=False) + \"\\n\"\n",
    "    except ValueError:\n",
    "        return \"테이블을 찾을 수 없습니다.\"\n",
    "\n",
    "def extract_text_and_tables_from_multiple_ranges(soup, ranges):\n",
    "    \"\"\" 여러 개의 특정 div 또는 a 범위 내에서 span 태그의 텍스트와 table 태그의 데이터를 수집 \"\"\"\n",
    "    combined_texts = []\n",
    "    table_count = 1\n",
    "    \n",
    "    for start_id, end_id in ranges:\n",
    "        start_element = soup.find(lambda tag: (tag.name == \"div\" or tag.name == \"a\") and tag.get(\"id\") == start_id)\n",
    "        end_element = soup.find(lambda tag: (tag.name == \"div\" or tag.name == \"a\") and tag.get(\"id\") == end_id)\n",
    "\n",
    "        if not start_element or not end_element:\n",
    "            print(f\"지정한 ID의 div 또는 a 태그를 찾을 수 없습니다: {start_id} - {end_id}\")\n",
    "            continue\n",
    "\n",
    "        all_elements = soup.find_all(lambda tag: tag.name == \"div\" or tag.name == \"a\")\n",
    "        start_index = all_elements.index(start_element)\n",
    "        end_index = all_elements.index(end_element)\n",
    "        div_count = end_index - start_index + 1\n",
    "        print(f\"'{start_id}'부터 '{end_id}'까지의 div 또는 a 개수: {div_count}\")\n",
    "\n",
    "        for element in all_elements[start_index:end_index+1]:\n",
    "            table = element.find(\"table\")\n",
    "            if table:\n",
    "                table_text = extract_table_from_html(table, table_count)\n",
    "                combined_texts.append(f\"[TABLE {table_count}]\")\n",
    "                combined_texts.append(table_text)\n",
    "                table_count += 1\n",
    "                continue\n",
    "\n",
    "            # 테이블이 포함된 경우, 그 내부의 <span> 태그는 수집하지 않음\n",
    "            if element.find(\"table\"):\n",
    "                continue\n",
    "\n",
    "            spans = [span for span in element.find_all(\"span\") if not span.find_parent(\"table\")]\n",
    "            for span in spans:\n",
    "                text_content = span.get_text(strip=True)\n",
    "                combined_texts.extend(text_content.split(\",\"))\n",
    "    \n",
    "    return combined_texts\n",
    "\n",
    "def save_results_to_csv(combined_texts, output_path):\n",
    "    \"\"\" 수집된 문장 리스트와 테이블 목록을 하나의 CSV 파일로 저장 \"\"\"\n",
    "    df = pd.DataFrame({\"Content\": combined_texts})\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"결과가 CSV 파일로 저장되었습니다: {output_path}\")\n",
    "\n",
    "# 실행 예시\n",
    "html_path = r'C:\\Users\\RMARKET\\Desktop\\yong\\data_folder\\crawing_data\\result\\0000002488\\0000002488_10-K_Report_2.html'\n",
    "soup, ranges = extract_item_links(html_path)\n",
    "combined_texts = extract_text_and_tables_from_multiple_ranges(soup, ranges)\n",
    "output_csv_path = r'C:\\Users\\RMARKET\\Desktop\\yong\\data_folder\\crawing_data\\parsed_data\\0000004127\\output2.csv'\n",
    "save_results_to_csv(combined_texts, output_csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
