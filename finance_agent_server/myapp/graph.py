import os
import json
from pathlib import Path
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

# RAG ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¦¬
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain_core.documents import Document
from langchain.tools.retriever import create_retriever_tool
import re

# retriever ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# mongoDB ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from pymongo import MongoClient
from datetime import datetime, timedelta

from django.shortcuts import render
from django.http import JsonResponse, FileResponse, HttpResponse

from pymongo import MongoClient

# ìˆ˜ìµë¥  ê°€ì ¸ì˜¤ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¦¬
import yfinance as yf
import pandas as pd
import requests

# agentë¥¼ êµ¬í˜„í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.tools import tool
from typing import List, Dict, Annotated
from langchain_experimental.utilities import PythonREPL
from langchain.agents import create_tool_calling_agent
from langchain.agents import AgentExecutor

# langsmith ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_teddynote import logging
logging.langsmith("STFO")

#ë§ˆí¬ë‹¤ìš´ renderingì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from django.utils.safestring import mark_safe
import markdown

#ì¶©ëŒ ë‚˜ëŠ” ê²ƒ í•´ê²°
os.environ['KMP_DUPLICATE_LIB_OK']='True'

# langgraph ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from typing import Annotated, Optional, List, Dict, TypedDict, Sequence, Literal
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel

from random import random
from IPython.display import Image, display

from langchain.output_parsers import JsonOutputKeyToolsParser


# ì¤‘ë³µìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒ ì„œë²„ ì—´ ë•Œ ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸°.#######################
load_dotenv()

cluster = MongoClient(os.environ.get("mongo")) # í´ëŸ¬ìŠ¤í„° 
db = cluster['userinfo'] # ìœ ì € ì •ë³´
conversations = db["conversations"] # ë‚´ê°€ ì§€ì •í•œ ì»¬ë ‰ì…˜ ì´ë¦„

api_key = os.getenv('OPENAI_API_KEY_sesac')

embeddings = OpenAIEmbeddings(openai_api_key=api_key)
news_vector_store = FAISS.load_local(rf'{Path(__file__).parents[0]}\vectorDB', embeddings=embeddings, allow_dangerous_deserialization=True,index_name = 'NewsData')
news_retriever = news_vector_store.as_retriever()

################################################################

class State(TypedDict):
    user_id: str  # ë¡œê·¸ì¸í•œ ì‚¬ìš©ì ID
    score : str # íˆ¬ì ì„±í–¥ ì²´í¬
    question: str  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸
    actions: List[str]  # ìˆ˜í–‰ëœ ì•¡ì…˜(ì‘ì—…) ê¸°ë¡
    context: List[Document]  # RAG ê¸°ë°˜ ê²€ìƒ‰ëœ ë¬¸ì„œ ë°ì´í„°
    research: str # ê¸ˆ s&p500 ë°ì´í„° ì •ë¦¬
    answer: str  # ì±—ë´‡ì˜ ì‘ë‹µ
    report: str  # ìµœì¢… ìƒì„±ëœ ë¦¬í¬íŠ¸ ë‚´ìš©

### Tools #################################
@tool
def finder_ticker(input_string:str) -> str:
# ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸ ì—¬ë¶€ í™•ì¸
    """
    ì…ë ¥ëœ ì¢…ëª© í‹°ì»¤ì½”ë“œì˜ í˜„ì¬ê°€ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    :param params: {"ticker_symbol": "AAPL"} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ì…ë ¥
    :return: í˜„ì¬ê°€ (float)
    """
    api_key = os.getenv('OPENAI_API_KEY_sesac')
    llm = ChatOpenAI(
        model_name='gpt-4o-mini',
        api_key=api_key,
        temperature = 0
    )

    prompt = PromptTemplate.from_template('''
    ì£¼ì–´ì§„ {message}ì—ì„œ íšŒì‚¬ì´ë¦„ì„ ì¶”ì¶œí•œ í›„ì— í‹°ì»¤ë¡œ ì¶œë ¥í•´ì¤˜. 
    ì¶œë ¥ í˜•íƒœëŠ” ë°˜ë“œì‹œ tickerë§Œ textë¡œ ë‚˜ì˜¤ë„ë¡ í•´ì¤˜.
    ''')
    # í”„ë¡¬í¬íŠ¸ ì¡°ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    chain = prompt | llm

    answer = chain.invoke({'message': input_string})

    try:
        # ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        stock = yf.Ticker(answer.content)
        
        # í˜„ì¬ê°€ ê°€ì ¸ì˜¤ê¸°
        current_price = stock.history(period="1d")['Close'].iloc[-1]
        
        return round(current_price, 2)  # ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼í•˜ì—¬ ë°˜í™˜

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# ê¸ˆê³¼ S&P 500 ì˜ íˆ¬ììœ¨ê°€ì ¸ì˜¤ê¸°ê¸° 
@tool
def compare_gold_prices(year_range: str) -> List[str]:
    """
    ê¸°ì¤€ë…„ë„ì™€ ë¹„êµë…„ë„ ê¸ˆ ì‹œì„¸ ë° S&P 500 ì§€ìˆ˜ ë° ìˆ˜ìµë¥ ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.
    
    ì…ë ¥ í˜•ì‹: "ì‹œì‘ë…„ë„,ì¢…ë£Œë…„ë„" (ì˜ˆ: "2020,2023")
    :return: ê¸ˆ & S&P 500 ê°€ê²© ë° ìˆ˜ìµë¥  ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    """
    # ë‚ ì§œ ë³€í™˜
    start_year, end_year = year_range.split(",")
    start_date = str(start_year.strip()) + '-01-01'
    end_date = str(end_year.strip()) + '-01-01'

    # ğŸ“Œ ê¸ˆ ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    start_gold_price = get_gold_price(start_date)
    end_gold_price = get_gold_price(end_date)

    # ğŸ“Œ S&P 500 ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    sp500_return, start_sp500_price, end_sp500_price = get_sp500_return(start_date, end_date)

    # ğŸ“Œ ê¸ˆ ìˆ˜ìµë¥  ê³„ì‚°
    gold_return = ((end_gold_price - start_gold_price) / start_gold_price * 100) if start_gold_price and end_gold_price else None

    return {
        # "ê¸°ì¤€ë…„ë„ ê¸ˆ ê°€ê²©": start_gold_price,
        # "ë¹„êµë…„ë„ ê¸ˆ ê°€ê²©": end_gold_price,
        # "ê¸°ì¤€ë…„ë„ S&P 500 ê°€ê²©": start_sp500_price,
        # "ë¹„êµë…„ë„ S&P 500 ê°€ê²©": end_sp500_price,
        "ê¸ˆ ìˆ˜ìµë¥  (%)": round(gold_return, 2) if gold_return is not None else "ë°ì´í„° ì—†ìŒ",
        "S&P 500 ìˆ˜ìµë¥  (%)": round(sp500_return, 2) if sp500_return is not None else "ë°ì´í„° ì—†ìŒ"
    }

def get_gold_price(date=""):
    """ íŠ¹ì • ë‚ ì§œì˜ ê¸ˆ ì‹œì„¸ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜ """
    api_key = "goldapi-2dxu3lsm6y7hdk6-io"
    symbol = "XAU"
    curr = "USD"
    
    url = f"https://www.goldapi.io/api/{symbol}/{curr}/{date}"
    
    headers = {
        "x-access-token": api_key,
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()
        return data.get("price", None)  # í˜„ì¬ ì˜¨ìŠ¤ë‹¹ ê¸ˆ ê°€ê²©
    except requests.exceptions.RequestException as e:
        print("Error:", str(e))
        return None

def get_sp500_return(start_date, end_date):
    """
    S&P 500 (^GSPC) ì§€ìˆ˜ì˜ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜

    :param start_date: ì¡°íšŒ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
    :param end_date: ì¡°íšŒ ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
    :return: S&P 500 ìˆ˜ìµë¥ , ì‹œì‘ ê°€ê²©, ì¢…ë£Œ ê°€ê²©
    """
    # S&P 500 í‹°ì»¤ (^GSPC) ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    sp500 = yf.Ticker("^GSPC")
    data = sp500.history(start=start_date, end=end_date)

    if data.empty:
        print(f"Error: No S&P 500 data found for {start_date} to {end_date}")
        return None, None, None

    # ì‹œì‘ ê°€ê²©ê³¼ ì¢…ë£Œ ê°€ê²© ê°€ì ¸ì˜¤ê¸°
    start_sp500_price = data["Close"].iloc[0]  # ì²«ë‚  ì¢…ê°€
    end_sp500_price = data["Close"].iloc[-1]   # ë§ˆì§€ë§‰ë‚  ì¢…ê°€

    # ìˆ˜ìµë¥  ê³„ì‚° (%)
    return_rate = ((end_sp500_price - start_sp500_price) / start_sp500_price) * 100

    return return_rate, start_sp500_price, end_sp500_price

# RAGë¥¼ ì‚¬ìš©í•´ì„œì„œ Newsdata ê°€ì ¸ì˜¤ê¸° (vecterDBì—ì„œ ê°€ì ¸ì˜¤ê¸°)
news_retriever_tool = create_retriever_tool(
    news_retriever,
    name = 'news_search',
    description='ë‰´ìŠ¤ ë°ì´í„°ì¤‘ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê²ƒì„ ì°¾ìŠµë‹ˆë‹¤.'
)

# íˆ¬ì ì„±í–¥ì„ MongoDBì—ì„œ ê°€ì ¸ì˜¤ê¸°
def take_investment_propensity(user_id: str) -> str:
    '''
    User_idë¥¼ ì…ë ¥í•˜ë©´ MongoDBì—ì„œ íˆ¬ì ì„±í–¥ì„ ë°›ìŠµë‹ˆë‹¤.
    '''
    conversation = conversations.find_one({"user_id": user_id})
    answer = conversation['score']
    return answer


# python ì‹¤í–‰ê¸°
@tool
def python_repl_tool(
    code: Annotated[str, "The python code to execute to generate your chart."],
    ):
    """Use this to execute python code. If you want to see the output of a value,
    you should print it out with `print(...)`. This is visible to the user."""
    result = ""
    try:
        result = PythonREPL().run(code)
    except BaseException as e:
        print(f"Failed to execute. Error: {repr(e)}")
    finally:
        return result

############################################################

## ìœ ì € ì•„ì´ë””ì™€ ì§ˆë¬¸ì€ í”„ë¡ íŠ¸ì—ì„œ apií˜•ì‹ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
user_id = "test123"
input_string = "ì–´ë–¤ ì£¼ì‹ì— íˆ¬ìí•˜ëŠ” ê²ƒì´ ì¢‹ì„ê¹Œ?"
##

# ì‚¬ìš©ì ë°ì´í„° (user_id, question)ì„ Stateì— ë„£ê¸°
def get_info(State):
    State['user_id'] = user_id
    State['question'] = input_string

    # íˆ¬ì ì„±í–¥ì„ MongoDBì—ì„œ ê°€ì ¸ì˜¤ê¸°
    conversation = conversations.find_one({"user_id": user_id})
    State['score'] = conversation['score']
    return State

# ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë°ì´í„°ê°€ ì¼ë°˜ ëŒ€í™”ë¡œ ëë‚ ì§€, ì—ì´ì „íŠ¸ê°€ í•„ìš”í•œì§€ êµ¬ë¶„
def compare_message(State):
    """'ì¼ë°˜ ëŒ€í™”'ì™€ 'ì—ì´ì „íŠ¸ í˜¸ì¶œ'í•˜ëŠ” ê²ƒì„ ë¶„ë¥˜í•˜ëŠ” ë…¸ë“œ"""
    llm = ChatOpenAI(
        model_name='gpt-4o-mini',
        api_key=api_key,
        temperature = 0,
    )
    parser = JsonOutputKeyToolsParser(keys=["category"])  # "category" í‚¤ë¥¼ ì¶”ì¶œí•˜ë„ë¡ ì„¤ì •
    prompt_template = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ 'ì£¼ì‹ íˆ¬ì ê´€ë ¨'ì¸ì§€ 'ì¼ë°˜ ì§ˆë¬¸'ì¸ì§€ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤."),
    ("user", "ì…ë ¥ ë©”ì‹œì§€: {input_message}")
    ]).with_structured_output(parser)  # JSON í˜•ì‹ ê°•ì œ
    response = llm.predict(prompt_template.format(input_message=State['question']))
    category = response.get("category", "general")  # ê¸°ë³¸ê°’ ì„¤ì •
    State['actions'] = State['actions'].append(response.content)
    return State

# ì¼ë°˜ ëŒ€í™”ì˜ ë
def chatbot_agent(State):
    '''ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€'''
    # conversation = conversations.find_one({'user_id': State['user_id']})
    # message = conversation['messages']
    llm = ChatOpenAI(
        model_name='gpt-4o',
        api_key=api_key,
        temperature = 0.5,
    )
    
    prompt = PromptTemplate.from_template(
        '''
        ë‹¹ì‹ ì€ íˆ¬ìì–´ë“œë°”ì´ì €ì…ë‹ˆë‹¤.
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”
        ì§ˆë¬¸ = {content}
        ''')
    
    chain = prompt | llm

    State['answer'] = chain.invoke({'content': State['question']})

    return State

# Supervisor ë§Œë“¤ê¸°
members = ['researcher_agent','report_agent','Investment_Recommendation_agent']
options = ['FINISH'] + members

class routeResponse(BaseModel):
    next: Literal['researcher_agent','report_agent','Investment_Recommendation_agent','FINISH']

def Supervisor(State):
    system_prompt = (
    "You are a supervisor tasked with managing a conversation between the"
    " following workers:  {members}. Given the following user request,"
    " respond with the worker to act next. Each worker will perform a"
    " task and respond with their results and status. When finished,"
    " respond with FINISH."
    )
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            (
                "system",
                "Given the conversation above, who should act next?"
                " Or should we FINISH? Select one of: {options}",
            ),
        ]
    ).partial(options=str(options), members=", ".join(members))


    llm = ChatOpenAI(model="gpt-4o-mini")

    supervisor_chain = (
        prompt
        | llm.with_structured_output(routeResponse)
    )
    result = supervisor_chain.invoke(State)  # LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì‘ì—… ê²°ì •
    return result.next  # ë‹¤ìŒ ì‹¤í–‰í•  ì—ì´ì „íŠ¸ ë°˜í™˜


################################   research tools ì œì‘

# research ë„êµ¬ë“¤ ì •ë¦¬.
research_tools = [ finder_ticker, compare_gold_prices,python_repl_tool]

# ë„êµ¬ë“¤ì˜ node ì •ë¦¬.
research_tool_node = ToolNode(research_tools)

# LLM ì„ ì–¸.
model_research = ChatOpenAI(
    model_name='gpt-4o-mini',
    api_key=os.getenv('OPENAI_API_KEY_sesac'),
    temperature=0,
).bind_tools(research_tools)

# retriever ì‚¬ìš©í•˜ëŠ” ë„êµ¬ ì •ë¦¬.
rag_tools = [news_retriever_tool]

# retriever node ì •ë¦¬ë¦¬
rag_tool_node = ToolNode(rag_tools)

# LLM ì„ ì–¸.
model_rag = ChatOpenAI(
    model_name='gpt-4o-mini',
    api_key=os.getenv('OPENAI_API_KEY_sesac'),
    temperature=0,).bind_tools(rag_tools)

############################################################################

def researcher_agent(State) -> Literal['tools','supervisor']:
    '''
    
    '''
    message = State['question']
    if model_research.invoke(message).tool_calls or model_rag.invoke(message).tool_calls:
        return 'tools'
    return 'supervisor'

def call_research_tools(State):
    rag_message = model_rag.invoke(State['question'])
    research_message = model_research.invoke(State['question'])
    State['context'] = rag_message
    State['research'] = research_message
    return State

def report_agent(State):
    """
    ì‹ ë¬¸ ê¸°ì‚¬ ë° ê¸ˆìœµ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ íˆ¬ì ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        State (dict): 'context' (ë‰´ìŠ¤ ë°ì´í„°), 'research' (ê¸ˆìœµ ë°ì´í„°)ë¥¼ í¬í•¨í•œ ìƒíƒœ ì •ë³´.
    """
    llm = ChatOpenAI(
        model_name='gpt-4o',
        api_key=api_key,
        temperature=0.2,
    )

    newsdata = State['context']
    finance_data = State['research']

    prompt = PromptTemplate.from_template(      
        '''
        ë„ˆëŠ” ì‹ ë¬¸ê¸°ì‚¬ ë°ì´í„°ì™€ ê³µì‹œì‹œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¼ìŒì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” ëŠ¥ìˆ™í•œ ì• ë„ë¦¬ìŠ¤íŠ¸ì´ë‹¤. 
        <ê°€ì´ë“œë¼ì¸>ì— ë”°ë¼ì„œ ê³µì‹ ë ¥ ìˆëŠ” ë³´ê³ ì„œì˜ í˜•íƒœë¡œ ì‘ì„±í•´ì¤˜.
        ì‹ ë¬¸ê¸°ì‚¬ ë°ì´í„° : {newsdata}
        ê³µì‹œì‹œ ë°ì´í„°: {finance_data}
        
                <ê°€ì´ë“œë¼ì¸>
                1. ì–¸ì œê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í–ˆëŠ”ì§€
                2. ì •ë³´ ì¶œì²˜
                3. í˜„ì¬ê°€ê²©
                4. ì˜ˆì¸¡ê°€ê²©(ë‚˜ì¤‘ì—)
                5. ì¶”ì„¸
                ì„±ì¥ì£¼ â†’ PSR, PEG, DCF í™œìš©
                ê°€ì¹˜ì£¼ â†’ PER, PBR í™œìš©
                ìì‚° ê¸°ë°˜ ê¸°ì—…(ì€í–‰, ë³´í—˜) â†’ PBR, ROE ê¸°ë°˜ í‰ê°€
                ë¶€ì±„ê°€ ë§ì€ ê¸°ì—… â†’ EV/EBITDA í™œìš©
                6. ì‹œê°€ì´ì•¡
                7. ê±°ë˜ì†Œ
                8. ì¼, ì£¼, ì›” í¼í¬ë¨¼ìŠ¤
                story : ìµœê·¼ì •ë³´ ìš”ì•½
                í˜¸ì¬ì™€ ì•…ì¬
                ì‹œì¥ ë™í–¥
                ë³€ë™ì„± ìˆ˜ì¹˜ì™€ ì™œ ì»¤ì¡ŒëŠ”ì§€ í™•ì¸
                ì™œ ìœ„í—˜í•œì§€ / ì˜ˆìƒë˜ëŠ” ìœ„í—˜ì´ ë¬´ì—‡ì¸ì§€?
                ì‹¤ì‹œê°„ ë‰´ìŠ¤
                9. ì£¼ìš” ì½”ë©˜íŠ¸
                10. buy/sell
                11. ê¸°ê°„ë™ì•ˆ ê¸ˆ ìˆ˜ìµë¥ ê³¼ s&p 500 íˆ¬ì ìˆ˜ìµë¥ 
                '''
    )

    chain = prompt | llm

    answer = chain.invoke({'newsdata':newsdata,'financedata':finance_data})

    State['report'] = answer.content

    return State

def Investment_Recommendation_agent(State):
    llm = ChatOpenAI(
            model_name='gpt-4o',
            api_key=api_key,
            temperature=0.2,
        )
    
    score = State['score'] 
    report = State['report']
    newsdata = State['context']
    finance_data = State['research']

    prompt = PromptTemplate.from_template(      
        '''
        ì•„ë˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ íˆ¬ì ì„±í–¥ì— ë§ëŠ” ìµœì ì˜ íˆ¬ì ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”.
        1. ê´€ë ¨ íˆ¬ì ë¦¬í¬íŠ¸ : {report}
        2. íˆ¬ì ì„±í–¥ : {score}
        3. ë‰´ìŠ¤ ë°ì´í„°: {newsdata}
        4. ê³µì‹œ ë°ì´í„°: {finance_data}

        : **íˆ¬ì ì„±í–¥ë³„ ì „ëµ ì œì•ˆ**:
        - **ìœ„í—˜ì„ í˜¸í˜•(ê³µê²©ì  íˆ¬ìì)**: ë†’ì€ ìˆ˜ìµ ê°€ëŠ¥ì„±ì„ ì¤‘ì‹œí•˜ë©° ë‹¨ê¸° ë³€ë™ì„±ì„ ê°ë‚´í•  ìˆ˜ ìˆëŠ” ì „ëµì„ ì¶”ì²œí•˜ì„¸ìš”.
        - **ì ê·¹í˜•**: ì£¼ì‹ê³¼ ê¸ˆì„ ê· í˜• ìˆê²Œ ì¡°í•©í•˜ì—¬ ì„±ì¥ì„±ê³¼ ì•ˆì •ì„±ì„ ëª¨ë‘ ê³ ë ¤í•œ ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”.
        - **ì„±ì¥í˜•**: ì¤‘ì¥ê¸°ì ì¸ ì„±ì¥ ì ì¬ë ¥ì´ ë†’ì€ ì‹œì¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì¡°ì–¸í•˜ì„¸ìš”.
        - **ì•ˆì •ì„±ì¥í˜•**: ì•ˆì „í•œ íˆ¬ì ì˜µì…˜ê³¼ í•¨ê»˜ ì¼ë¶€ ë¦¬ìŠ¤í¬ë¥¼ ê°ìˆ˜í•  ìˆ˜ ìˆëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
        - **ìœ„í—˜íšŒí”¼í˜•**: ì›ê¸ˆ ì†ì‹¤ ê°€ëŠ¥ì„±ì„ ìµœì†Œí™”í•˜ê³ , ê¸ˆ íˆ¬ì ë¹„ì¤‘ì„ ëŠ˜ë¦´ ìˆ˜ ìˆëŠ” ë³´ìˆ˜ì ì¸ ì „ëµì„ ì œì‹œí•˜ì„¸ìš”.

        íˆ¬ì ì„±í–¥ê³¼ ì‹œì¥ ë°ì´í„°ì— ë§ì¶° í˜„ì‹¤ì ì¸ íˆ¬ì ì¡°ì–¸ì„ ì‘ì„±í•˜ì„¸ìš”.
        ë§Œì•½ ì…ë ¥ëœ íˆ¬ì ì„±í–¥ê³¼ ë‹¤ë¥¸ ì„±í–¥ì˜ íˆ¬ì ë°©ë²•ì„ ìš”êµ¬í•˜ì—¬ë„ ë°˜ë“œì‹œ scoreì—ì„œ ì…ë ¥ëœ ì„±í–¥ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.
        '''
        ,
    )

    chain = prompt | llm

    answer = chain.invoke({'report':report,'score':score,'newsdata':newsdata,'finance_data':finance_data})

    State['answer'] = answer.content

    return State

graph = StateGraph(State)

graph.add_node("get_info", get_info)
graph.add_node("compare_message", compare_message)
graph.add_node("chatbot_agent", chatbot_agent)
graph.add_node("Supervisor", Supervisor)  # âœ… Supervisorë¥¼ ìƒìœ„ ì—ì´ì „íŠ¸ë¡œ ë“±ë¡
graph.add_node("researcher_agent", researcher_agent)
graph.add_node("call_research_tools", call_research_tools)
graph.add_node("report_agent", report_agent)
graph.add_node("Investment_Recommendation_agent", Investment_Recommendation_agent)

# ì´ˆê¸° Edge ì„¤ì •
graph.add_edge(START, "get_info")  # ì‹œì‘ -> ì‚¬ìš©ì ì •ë³´ ë¡œë“œ
graph.add_edge("get_info", "compare_message")  # ì •ë³´ ë¡œë“œ í›„ ì§ˆë¬¸ ë¶„ì„

# ì§ˆë¬¸ ìœ í˜• ë¶„ê¸°
graph.add_conditional_edges(
    "compare_message", 
    lambda state: "Supervisor" if "investment" in state["actions"] else "chatbot_agent"
)

# ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
graph.add_edge("chatbot_agent", END)  # ì¼ë°˜ ëŒ€í™” í›„ ì¢…ë£Œ

# Supervisorë¥¼ í†µí•œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹¤í–‰
graph.add_conditional_edges(
    "Supervisor",
    lambda state: Supervisor(state),  # Supervisorê°€ ì§ì ‘ íŒë‹¨í•˜ì—¬ ë‹¤ìŒ ì—ì´ì „íŠ¸ ì‹¤í–‰
)

# ì—°êµ¬ ì—ì´ì „íŠ¸ ì‹¤í–‰ í›„ ì—°êµ¬ ë„êµ¬ ì‹¤í–‰ (í•„ìš”í•œ ê²½ìš°)
graph.add_edge("researcher_agent", "call_research_tools", condition=lambda state: state["actions"] == "tools")
graph.add_edge("call_research_tools", "Supervisor")  # ì—°êµ¬ ì™„ë£Œ í›„ Supervisorë¡œ ë³µê·€

# ë³´ê³ ì„œ ë° íˆ¬ì ì¶”ì²œ ì‹¤í–‰
graph.add_edge("report_agent", "Supervisor")  # ë³´ê³ ì„œ ìƒì„± í›„ Supervisorë¡œ ë³µê·€
graph.add_edge("Investment_Recommendation_agent", END)  # íˆ¬ì ì¶”ì²œ í›„ ì¢…ë£Œ

# ê·¸ë˜í”„ ì‹¤í–‰
graph.compile()



